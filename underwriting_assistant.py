import streamlit as st
import os
import json
from datetime import datetime
from pathlib import Path
import hashlib
from typing import List, Dict, Any, Tuple
import io

# Document & Image Processing
from pdf2image import convert_from_path
from docx import Document as DocxDocument
from PIL import Image, ImageDraw
import numpy as np

# OCR
import easyocr
import pytesseract

# RAG Components
from sentence_transformers import SentenceTransformer
import faiss
from rank_bm25 import BM25Okapi
import requests
import pandas as pd

# ===========================
# Configuration
# ===========================

VERSION = "4.1.0-PIL"
APP_TITLE = "Underwriting Repository - RAG with Handwriting Recognition"

# Preset Datasets (Default cases to load)
PRESET_DATASETS = [
    "Cargo - Agnes Fisheries - CE's notes for Yr 2021-22.pdf",
    "Hull - Marco Polo_Memo.pdf",
    "Cargo - Mitsui Co summary with CE's QA 29.8.22.docx"
]

# API Configuration
DEFAULT_API_KEY = "sk-99bba2ce117444e197270f17d303e74f"
API_BASE = "https://api.deepseek.com/v1"
API_MODEL = "deepseek-chat"

# RAG Configuration
CHUNK_SIZE = 512
CHUNK_OVERLAP = 128
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
TOP_K_RETRIEVAL = 10
TOP_K_RERANK = 3

# OCR Configuration
TESSERACT_LANG = "eng+chi_sim"
EASYOCR_LANGS = ['en', 'ch_sim']

# Directories
DATA_DIR = Path("data")
UPLOADS_DIR = DATA_DIR / "uploads"
PROCESSED_DIR = DATA_DIR / "processed"
VECTOR_DB_DIR = DATA_DIR / "vector_db"
OCR_RESULTS_DIR = DATA_DIR / "ocr_results"

# ===========================
# Directory Setup
# ===========================

def ensure_directories():
    """Create necessary directories"""
    for dir_path in [DATA_DIR, UPLOADS_DIR, PROCESSED_DIR, VECTOR_DB_DIR, OCR_RESULTS_DIR]:
        dir_path.mkdir(parents=True, exist_ok=True)

# ===========================
# Model Loading
# ===========================

@st.cache_resource
def load_embedding_model():
    """Load embedding model"""
    return SentenceTransformer(EMBEDDING_MODEL)

@st.cache_resource
def load_ocr_reader():
    """Load EasyOCR reader"""
    return easyocr.Reader(EASYOCR_LANGS, gpu=False)

# ===========================
# Step 1: Text Extraction & OCR
# ===========================

def convert_pdf_to_images(file_path: Path) -> List[Image.Image]:
    """Convert PDF pages to images for OCR processing"""
    try:
        # Use poppler for PDF conversion
        images = convert_from_path(str(file_path), dpi=200)
        return images
    except Exception as e:
        st.error(f"PDF to image conversion error: {e}")
        return []

def perform_hybrid_ocr(image: Image.Image, reader: easyocr.Reader, page_idx: int, doc_id: str) -> Dict:
    """
    Perform Hybrid OCR:
    1. Tesseract for the full page (Best for printed text blocks)
    2. EasyOCR for detection (Best for handwriting/marginalia)
    """
    page_result = {
        'page_num': page_idx + 1,
        'full_printed_text': "",
        'handwritten_regions': []
    }

    try:
        # 1. Full Page Tesseract (Fast, Printed Text)
        full_text = pytesseract.image_to_string(image, lang=TESSERACT_LANG)
        page_result['full_printed_text'] = full_text.strip()

        # 2. EasyOCR for granular detection (Handwriting/Regions)
        image_np = np.array(image)
        # detail=1 returns bounding box, text, and confidence
        detections = reader.readtext(image_np)

        for idx, (bbox_points, text, conf) in enumerate(detections):
            # EasyOCR returns poly points [[x1,y1],[x2,y2]...]. Convert to Box (x, y, w, h)
            x_coords = [p[0] for p in bbox_points]
            y_coords = [p[1] for p in bbox_points]
            
            x_min, x_max = int(min(x_coords)), int(max(x_coords))
            y_min, y_max = int(min(y_coords)), int(max(y_coords))
            w = x_max - x_min
            h = y_max - y_min

            # Filter out tiny noise
            if w < 20 or h < 10:
                continue

            # Crop the region for the UI to display
            try:
                crop = image.crop((x_min, y_min, x_max, y_max))
                region_id = idx
                region_img_path = OCR_RESULTS_DIR / f"{doc_id}_p{page_idx+1}_r{region_id}.png"
                crop.save(region_img_path)

                page_result['handwritten_regions'].append({
                    'region_id': region_id,
                    'bbox': (x_min, y_min, w, h),
                    'text': text,
                    'confidence': float(conf),
                    'image_path': str(region_img_path)
                })
            except Exception as crop_err:
                print(f"Skipping invalid crop: {crop_err}")
                continue

    except Exception as e:
        st.error(f"OCR Error on page {page_idx+1}: {e}")

    return page_result

def process_pdf_with_ocr(file_path: Path, doc_id: str, reader: easyocr.Reader) -> Dict:
    """
    Complete PDF processing pipeline
    """
    results = {
        'doc_id': doc_id,
        'filename': file_path.name,
        'pages': []
    }
    
    # Convert to images
    images = convert_pdf_to_images(file_path)
    
    if not images:
        return results
    
    progress_bar = st.progress(0)
    
    for page_idx, page_image in enumerate(images):
        # Process page
        page_result = perform_hybrid_ocr(page_image, reader, page_idx, doc_id)
        results['pages'].append(page_result)
        
        # Update progress
        progress_bar.progress((page_idx + 1) / len(images))
    
    # Save OCR results
    results_path = OCR_RESULTS_DIR / f"{doc_id}_results.json"
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    return results

def extract_text_from_docx(file_path: Path) -> str:
    """Extract text from DOCX"""
    try:
        doc = DocxDocument(file_path)
        text_parts = []
        
        for para in doc.paragraphs:
            if para.text.strip():
                text_parts.append(para.text)
        
        for table in doc.tables:
            for row in table.rows:
                row_data = [cell.text.strip() for cell in row.cells]
                text_parts.append(" | ".join(row_data))
        
        return "\n".join(text_parts)
    except Exception as e:
        st.error(f"DOCX extraction error: {e}")
        return ""

# ===========================
# Step 2: Text Chunking
# ===========================

def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[Dict]:
    """Split text into overlapping chunks"""
    if not text:
        return []
    
    chunks = []
    start = 0
    chunk_id = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk_text = text[start:end]
        
        # Try to break at sentence boundary
        if end < len(text):
            last_period = chunk_text.rfind('.')
            last_newline = chunk_text.rfind('\n')
            break_point = max(last_period, last_newline)
            
            if break_point > chunk_size * 0.5:
                end = start + break_point + 1
                chunk_text = text[start:end]
        
        chunks.append({
            'chunk_id': chunk_id,
            'text': chunk_text.strip(),
            'start_char': start,
            'end_char': end
        })
        
        start = end - overlap
        chunk_id += 1
    
    return chunks

# ===========================
# Step 3: Embedding Generation
# ===========================

def generate_embeddings(texts: List[str], model: SentenceTransformer) -> np.ndarray:
    """Generate embeddings for text chunks"""
    embeddings = model.encode(texts, show_progress_bar=False, convert_to_numpy=True)
    return embeddings

# ===========================
# Step 4: Vector Database
# ===========================

class VectorDatabase:
    """FAISS-based vector database for semantic search"""
    
    def __init__(self, dimension: int = 384):
        self.dimension = dimension
        self.index = faiss.IndexFlatIP(dimension)
        self.chunks = []
        self.doc_metadata = {}
    
    def add_documents(self, chunks: List[Dict], embeddings: np.ndarray, 
                      doc_id: str, doc_name: str, source_type: str = "printed"):
        """Add document chunks to vector database"""
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings)
        
        for chunk, embedding in zip(chunks, embeddings):
            chunk['doc_id'] = doc_id
            chunk['doc_name'] = doc_name
            chunk['source_type'] = source_type
            self.chunks.append(chunk)
        
        if doc_id not in self.doc_metadata:
            self.doc_metadata[doc_id] = {
                'doc_name': doc_name,
                'num_chunks': 0,
                'printed_chunks': 0,
                'handwritten_chunks': 0
            }
        
        self.doc_metadata[doc_id]['num_chunks'] += len(chunks)
        if source_type == "printed":
            self.doc_metadata[doc_id]['printed_chunks'] += len(chunks)
        else:
            self.doc_metadata[doc_id]['handwritten_chunks'] += len(chunks)
    
    def search(self, query_embedding: np.ndarray, top_k: int = TOP_K_RETRIEVAL) -> List[Tuple[Dict, float]]:
        """Search for similar chunks"""
        if self.index.ntotal == 0:
            return []
        
        query_embedding = query_embedding.reshape(1, -1).astype('float32')
        faiss.normalize_L2(query_embedding)
        
        similarities, indices = self.index.search(query_embedding, min(top_k, self.index.ntotal))
        
        results = []
        for similarity, idx in zip(similarities[0], indices[0]):
            if idx < len(self.chunks):
                results.append((self.chunks[idx], float(similarity)))
        
        return results
    
    def get_stats(self) -> Dict:
        """Get database statistics"""
        total_printed = sum(meta['printed_chunks'] for meta in self.doc_metadata.values())
        total_handwritten = sum(meta['handwritten_chunks'] for meta in self.doc_metadata.values())
        
        return {
            'total_chunks': self.index.ntotal,
            'total_documents': len(self.doc_metadata),
            'printed_chunks': total_printed,
            'handwritten_chunks': total_handwritten,
            'dimension': self.dimension
        }
    
    def save(self, path: Path):
        """Save vector database"""
        path.mkdir(parents=True, exist_ok=True)
        faiss.write_index(self.index, str(path / "faiss.index"))
        
        with open(path / "chunks.json", 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False, indent=2)
        
        with open(path / "metadata.json", 'w', encoding='utf-8') as f:
            json.dump(self.doc_metadata, f, indent=2)
    
    def load(self, path: Path) -> bool:
        """Load vector database"""
        try:
            self.index = faiss.read_index(str(path / "faiss.index"))
            
            with open(path / "chunks.json", 'r', encoding='utf-8') as f:
                self.chunks = json.load(f)
            
            with open(path / "metadata.json", 'r', encoding='utf-8') as f:
                self.doc_metadata = json.load(f)
            
            return True
        except:
            return False

# ===========================
# Step 5-6: BM25 Reranking
# ===========================

class BM25Reranker:
    """BM25 reranker for retrieved chunks"""
    
    def __init__(self):
        self.bm25 = None
    
    def fit(self, texts: List[str]):
        tokenized = [text.lower().split() for text in texts]
        self.bm25 = BM25Okapi(tokenized)
    
    def rerank(self, query: str, candidates: List[Tuple[Dict, float]], 
              top_k: int = TOP_K_RERANK) -> List[Tuple[Dict, float]]:
        """Rerank candidates using BM25"""
        if not candidates:
            return []
        
        texts = [chunk['text'] for chunk, _ in candidates]
        self.fit(texts)
        
        tokenized_query = query.lower().split()
        bm25_scores = self.bm25.get_scores(tokenized_query)
        
        combined = []
        for (chunk, vec_score), bm25_score in zip(candidates, bm25_scores):
            # Normalize and Combine (0.6 Vector + 0.4 Keyword)
            combined_score = 0.6 * vec_score + 0.4 * (bm25_score / (max(bm25_scores) + 1e-6))
            combined.append((chunk, combined_score))
        
        combined.sort(key=lambda x: x[1], reverse=True)
        return combined[:top_k]

# ===========================
# Step 7: Prompt Template
# ===========================

def create_rag_prompt(query: str, context_chunks: List[Tuple[Dict, float]]) -> str:
    """Create RAG prompt with retrieved context"""
    context_parts = []
    for idx, (chunk, score) in enumerate(context_chunks, 1):
        source_label = "üñ®Ô∏è PRINTED" if chunk.get('source_type') == 'printed' else "‚úçÔ∏è EXTRACTED"
        context_parts.append(f"""
[Source {idx}] {source_label}
Document: {chunk['doc_name']}
Chunk ID: {chunk['chunk_id']}
Relevance: {score:.3f}
---
{chunk['text']}
---
""")
    
    context_text = "\n".join(context_parts)
    
    prompt = f"""You are a professional underwriting assistant. Answer based on the retrieved context below.

RETRIEVED CONTEXT:
{context_text}

USER QUESTION:
{query}

INSTRUCTIONS:
1. Answer based ONLY on the provided context
2. Cite source number and type (PRINTED/EXTRACTED)
3. If insufficient information, state clearly
4. Be specific with amounts, dates, terms
5. Note handwritten sources may have OCR errors

ANSWER:"""
    
    return prompt

# ===========================
# Step 8: LLM API Call
# ===========================

def call_llm_api(prompt: str) -> str:
    """Call LLM API for answer generation"""
    try:
        api_key = st.session_state.get('api_key', DEFAULT_API_KEY)
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": API_MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "max_tokens": 2000
        }
        
        response = requests.post(
            f"{API_BASE}/chat/completions",
            headers=headers,
            json=payload,
            timeout=60
        )
        
        response.raise_for_status()
        return response.json()['choices'][0]['message']['content']
        
    except Exception as e:
        return f"Error: {e}"

# ===========================
# Complete RAG Pipeline
# ===========================

def rag_query(query: str, vector_db: VectorDatabase, embedding_model: SentenceTransformer,
              reranker: BM25Reranker) -> Dict:
    """Complete RAG pipeline execution"""
    # Step 5: Query embedding
    query_embedding = embedding_model.encode([query])[0]
    
    # Step 6: Retrieve top-k chunks
    candidates = vector_db.search(query_embedding, top_k=TOP_K_RETRIEVAL)
    
    if not candidates:
        return {
            'answer': "No relevant documents found.",
            'sources': [],
            'num_candidates': 0
        }
    
    # Step 7: Rerank
    reranked = reranker.rerank(query, candidates, top_k=TOP_K_RERANK)
    
    # Step 8: Create prompt and call LLM
    prompt = create_rag_prompt(query, reranked)
    answer = call_llm_api(prompt)
    
    sources = [
        {
            'doc_name': chunk['doc_name'],
            'chunk_id': chunk['chunk_id'],
            'source_type': chunk.get('source_type', 'unknown'),
            'score': score,
            'text_preview': chunk['text'][:200] + "..."
        }
        for chunk, score in reranked
    ]
    
    return {
        'answer': answer,
        'sources': sources,
        'num_candidates': len(candidates)
    }

# ===========================
# Preset Dataset Loading
# ===========================

def load_preset_datasets(vector_db: VectorDatabase, embedding_model: SentenceTransformer, 
                         ocr_reader: easyocr.Reader):
    """
    Load and process preset datasets
    """
    st.info("üîÑ Loading preset datasets...")
    
    # Check if already loaded
    if VECTOR_DB_DIR.exists() and (VECTOR_DB_DIR / "faiss.index").exists():
        if vector_db.load(VECTOR_DB_DIR):
            st.success("‚úÖ Loaded existing vector database")
            return
    
    # Process each preset file
    for filename in PRESET_DATASETS:
        file_path = UPLOADS_DIR / filename
        
        if not file_path.exists():
            st.warning(f"‚ö†Ô∏è Preset file not found: {filename}. Please place in {UPLOADS_DIR}")
            continue
        
        st.info(f"üìÑ Processing: {filename}")
        
        doc_id = hashlib.md5(filename.encode()).hexdigest()[:8]
        
        # Step 1: Extract text
        if filename.endswith('.pdf'):
            # OCR processing for PDF
            ocr_results = process_pdf_with_ocr(file_path, doc_id, ocr_reader)
            
            # Extract printed text
            printed_text = "\n\n".join([
                page['full_printed_text'] for page in ocr_results['pages']
            ])
            
            # Extract detected regions (Handwritten/Mixed)
            handwritten_text = "\n\n".join([
                region['text'] 
                for page in ocr_results['pages'] 
                for region in page['handwritten_regions']
            ])
            
            # Step 2-4: Chunk, embed, and index printed text
            if printed_text.strip():
                printed_chunks = chunk_text(printed_text)
                if printed_chunks:
                    printed_texts = [c['text'] for c in printed_chunks]
                    printed_embeddings = generate_embeddings(printed_texts, embedding_model)
                    vector_db.add_documents(printed_chunks, printed_embeddings, doc_id, filename, "printed")
            
            # Step 2-4: Chunk, embed, and index handwritten text
            if handwritten_text.strip():
                hw_chunks = chunk_text(handwritten_text)
                if hw_chunks:
                    hw_texts = [c['text'] for c in hw_chunks]
                    hw_embeddings = generate_embeddings(hw_texts, embedding_model)
                    vector_db.add_documents(hw_chunks, hw_embeddings, doc_id, filename, "handwritten")
        
        elif filename.endswith('.docx'):
            # Direct text extraction for DOCX
            text = extract_text_from_docx(file_path)
            if text.strip():
                chunks = chunk_text(text)
                if chunks:
                    texts = [c['text'] for c in chunks]
                    embeddings = generate_embeddings(texts, embedding_model)
                    vector_db.add_documents(chunks, embeddings, doc_id, filename, "printed")
    
    # Save vector database
    vector_db.save(VECTOR_DB_DIR)
    st.success("‚úÖ All preset datasets processed and indexed")

# ===========================
# Streamlit UI
# ===========================

def render_header():
    st.markdown(f"""
    <div style='background: linear-gradient(90deg, #1e3a8a 0%, #7c3aed 100%); 
                padding: 2rem; border-radius: 10px; margin-bottom: 2rem;'>
        <h1 style='color: white; margin: 0;'>üìã {APP_TITLE}</h1>
        <p style='color: #e0e7ff; margin-top: 0.5rem;'>Version {VERSION}</p>
    </div>
    """, unsafe_allow_html=True)

def main():
    st.set_page_config(
        page_title="Underwriting RAG",
        page_icon="üìã",
        layout="wide"
    )
    
    ensure_directories()
    render_header()
    
    # Initialize session state
    if 'vector_db' not in st.session_state:
        st.session_state.vector_db = VectorDatabase()
    
    if 'reranker' not in st.session_state:
        st.session_state.reranker = BM25Reranker()
    
    if 'datasets_loaded' not in st.session_state:
        st.session_state.datasets_loaded = False
    
    # Load models
    embedding_model = load_embedding_model()
    ocr_reader = load_ocr_reader()
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        
        # API Key
        with st.expander("üîë API Key"):
            api_key = st.text_input("DeepSeek API Key:", type="password", value=DEFAULT_API_KEY)
            if st.button("Save Key"):
                st.session_state.api_key = api_key
                st.success("‚úì Saved")
        
        st.markdown("---")
        
        # Load preset datasets
        if not st.session_state.datasets_loaded:
            if st.button("üì• Load Preset Datasets", type="primary"):
                with st.spinner("Processing datasets..."):
                    load_preset_datasets(st.session_state.vector_db, embedding_model, ocr_reader)
                st.session_state.datasets_loaded = True
                st.rerun()
        else:
            st.success("‚úÖ Datasets Loaded")
            if st.button("üîÑ Reload"):
                st.session_state.datasets_loaded = False
                st.session_state.vector_db = VectorDatabase()
                st.rerun()
        
        st.markdown("---")
        
        # Stats
        stats = st.session_state.vector_db.get_stats()
        st.metric("Documents", stats['total_documents'])
        st.metric("Total Chunks", stats['total_chunks'])
        col1, col2 = st.columns(2)
        with col1:
            st.metric("üñ®Ô∏è Printed", stats['printed_chunks'])
        with col2:
            st.metric("‚úçÔ∏è Extracted", stats['handwritten_chunks'])
    
    # Main tabs
    tab1, tab2, tab3, tab4 = st.tabs([
        "üí¨ AI Chat (RAG)", 
        "üìÑ Document Viewer", 
        "üóÑÔ∏è Vector Database", 
        "‚úçÔ∏è OCR Results"
    ])
    
    # TAB 1: AI Chat with RAG
    with tab1:
        st.header("üí¨ AI Chat with RAG Pipeline")
        
        if stats['total_documents'] == 0:
            st.warning("‚ö†Ô∏è Please load preset datasets first")
        else:
            st.markdown("""
            **RAG Pipeline Steps:**
            - **Step 5**: Query vectorization
            - **Step 6**: Semantic search (retrieve top-k)
            - **Step 7**: BM25 reranking + context assembly
            - **Step 8**: LLM generation
            """)
            
            query = st.text_area(
                "Enter your question:",
                placeholder="e.g., What are the retention terms in Agnes Fisheries case?",
                height=100
            )
            
            if st.button("üîç Search & Answer", type="primary"):
                if query:
                    with st.spinner("Running RAG pipeline..."):
                        result = rag_query(
                            query,
                            st.session_state.vector_db,
                            embedding_model,
                            st.session_state.reranker
                        )
                        
                        st.markdown("### üí° Answer")
                        st.markdown(result['answer'])
                        
                        st.markdown("---")
                        st.markdown("### üìö Retrieved Sources")
                        
                        for idx, source in enumerate(result['sources'], 1):
                            source_icon = "üñ®Ô∏è" if source['source_type'] == 'printed' else "‚úçÔ∏è"
                            with st.expander(
                                f"{source_icon} Source {idx}: {source['doc_name']} (Score: {source['score']:.3f})"
                            ):
                                st.markdown(f"**Type:** {source['source_type'].upper()}")
                                st.markdown(f"**Chunk ID:** {source['chunk_id']}")
                                st.markdown(f"**Preview:**\n{source['text_preview']}")
    
    # TAB 2: Document Viewer
    with tab2:
        st.header("üìÑ Document Viewer (Original Files)")
        st.markdown("View original documents and their extracted content")
        
        ocr_results_files = list(OCR_RESULTS_DIR.glob("*_results.json"))
        
        if not ocr_results_files:
            st.info("No processed documents yet")
        else:
            selected_file = st.selectbox(
                "Select document:",
                [f.stem.replace("_results", "") for f in ocr_results_files]
            )
            
            if selected_file:
                results_path = OCR_RESULTS_DIR / f"{selected_file}_results.json"
                
                with open(results_path, 'r', encoding='utf-8') as f:
                    ocr_results = json.load(f)
                
                st.subheader(f"üìÑ {ocr_results['filename']}")
                
                for page in ocr_results['pages']:
                    with st.expander(f"Page {page['page_num']}", expanded=False):
                        st.markdown("#### üñ®Ô∏è Printed Text (Full Page)")
                        st.text_area(
                            "Full page text:",
                            value=page['full_printed_text'],
                            height=200,
                            key=f"printed_{page['page_num']}"
                        )
                        
                        if page['handwritten_regions']:
                            st.markdown("#### ‚úçÔ∏è Detected Regions (EasyOCR)")
                            st.info(f"Found {len(page['handwritten_regions'])} region(s)")
                            
                            for region in page['handwritten_regions']:
                                col1, col2 = st.columns([1, 2])
                                
                                with col1:
                                    if Path(region['image_path']).exists():
                                        img = Image.open(region['image_path'])
                                        st.image(img, caption=f"Region {region['region_id']}", use_column_width=True)
                                
                                with col2:
                                    st.markdown(f"**Region {region['region_id']}**")
                                    st.markdown(f"**Bounding Box:** {region['bbox']}")
                                    st.markdown(f"**Confidence:** {region.get('confidence', 0.0):.2f}")
                                    st.markdown("**OCR Transcription:**")
                                    st.code(region['text'], language=None)
    
    # TAB 3: Vector Database
    with tab3:
        st.header("üóÑÔ∏è Vector Database Explorer")
        
        stats = st.session_state.vector_db.get_stats()
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total Chunks", stats['total_chunks'])
        with col2:
            st.metric("Documents", stats['total_documents'])
        with col3:
            st.metric("üñ®Ô∏è Printed", stats['printed_chunks'])
        with col4:
            st.metric("‚úçÔ∏è Extracted", stats['handwritten_chunks'])
        
        st.markdown("---")
        
        st.subheader("Document Breakdown")
        
        doc_data = []
        for doc_id, meta in st.session_state.vector_db.doc_metadata.items():
            doc_data.append({
                'Document': meta['doc_name'],
                'Total Chunks': meta['num_chunks'],
                'üñ®Ô∏è Printed': meta['printed_chunks'],
                '‚úçÔ∏è Extracted': meta['handwritten_chunks']
            })
        
        if doc_data:
            df = pd.DataFrame(doc_data)
            st.dataframe(df, use_container_width=True)
        
        st.markdown("---")
        
        st.subheader("Sample Chunks (First 10)")
        
        if st.session_state.vector_db.chunks:
            sample_size = min(10, len(st.session_state.vector_db.chunks))
            samples = st.session_state.vector_db.chunks[:sample_size]
            
            for idx, chunk in enumerate(samples, 1):
                source_icon = "üñ®Ô∏è" if chunk.get('source_type') == 'printed' else "‚úçÔ∏è"
                with st.expander(f"{source_icon} Chunk {idx}: {chunk['doc_name']} (ID: {chunk['chunk_id']})"):
                    st.json({
                        'chunk_id': chunk['chunk_id'],
                        'doc_name': chunk['doc_name'],
                        'source_type': chunk.get('source_type', 'unknown'),
                        'text': chunk['text'][:500] + "..."
                    })
    
    # TAB 4: OCR Results
    with tab4:
        st.header("‚úçÔ∏è OCR Processing Results")
        
        st.markdown("""
        View detailed OCR processing results:
        - Text block detection (EasyOCR)
        - OCR transcriptions
        """)
        
        ocr_results_files = list(OCR_RESULTS_DIR.glob("*_results.json"))
        
        if not ocr_results_files:
            st.info("No OCR results yet. Load preset datasets to process documents.")
        else:
            for results_file in ocr_results_files:
                with open(results_file, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                
                with st.expander(f"üìÑ {results['filename']}", expanded=False):
                    st.markdown(f"**Document ID:** `{results['doc_id']}`")
                    st.markdown(f"**Total Pages:** {len(results['pages'])}")
                    
                    for page in results['pages']:
                        st.markdown(f"#### Page {page['page_num']}")
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("Printed Text Length", len(page['full_printed_text']))
                        with col2:
                            st.metric("Detected Regions", len(page['handwritten_regions']))
                        
                        if page['handwritten_regions']:
                            st.markdown("**Detected Regions:**")
                            
                            for region in page['handwritten_regions']:
                                col_a, col_b = st.columns([1, 3])
                                
                                with col_a:
                                    if Path(region['image_path']).exists():
                                        img = Image.open(region['image_path'])
                                        st.image(img, width=200)
                                
                                with col_b:
                                    st.markdown(f"**Region {region['region_id']}**")
                                    st.markdown(f"Bbox: {region['bbox']}")
                                    st.code(region['text'])
                        
                        st.markdown("---")

if __name__ == "__main__":
    main()

```
