#!/usr/bin/env python3
"""
Enhanced Underwriting Assistant v2.9.0
æ ¸å¿ƒæ”¹è¿›ï¼š
1. Integrated Analysis Report ä»¥è¡¨æ ¼å½¢å¼å±•ç¤º
2. æ”¯æŒæŒ‰ä¿é™©ç±»å‹ã€å®¢æˆ·åç§°ã€æ‰¿ä¿å¹´åº¦ç­›é€‰
3. Handwriting Translation æ˜¾ç¤ºå›¾ç‰‡+ç¿»è¯‘+è¯†åˆ«åº¦ç™¾åˆ†æ¯”
"""

import streamlit as st
import os
import json
import hashlib
import base64
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import io
import re

# å°è¯•å¯¼å…¥ PDF å¤„ç†åº“
try:
    import fitz  # PyMuPDF
    HAS_PYMUPDF = True
except ImportError:
    HAS_PYMUPDF = False

try:
    import PyPDF2
    HAS_PYPDF2 = True
except ImportError:
    HAS_PYPDF2 = False

try:
    from PIL import Image
    HAS_PIL = True
except ImportError:
    HAS_PIL = False

try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False

# =============================================================================
# æ ¸å¿ƒé…ç½®
# =============================================================================

VERSION = "2.9.0"
APP_TITLE = "Enhanced Underwriting Assistant - Table View System"

# DeepSeek API é…ç½®
API_BASE = "https://api.deepseek.com/v1"
API_MODEL = "deepseek-chat"
DEFAULT_API_KEY = os.getenv("DEEPSEEK_API_KEY", "sk-99bba2ce117444e197270f17d303e74f")

# æ•°æ®ç›®å½•ç»“æ„
DATA_DIR = "data"
WORKSPACES_DIR = os.path.join(DATA_DIR, "workspaces")
EMBEDDINGS_DIR = os.path.join(DATA_DIR, "embeddings")
ANALYSIS_DIR = os.path.join(DATA_DIR, "analysis")
REVIEW_DIR = os.path.join(DATA_DIR, "review_queue")
AUDIT_DIR = os.path.join(DATA_DIR, "audit_logs")
CONFIG_DIR = os.path.join(DATA_DIR, "config")

# åˆå§‹æ•°æ®é›†
INITIAL_DATASET = "Hull_MSC_Memo.pdf"

# æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
SUPPORTED_FORMATS = {
    "pdf": "ğŸ“„",
    "docx": "ğŸ“",
    "doc": "ğŸ“",
    "txt": "ğŸ“ƒ",
    "xlsx": "ğŸ“Š",
    "xls": "ğŸ“Š",
    "png": "ğŸ–¼ï¸",
    "jpg": "ğŸ–¼ï¸",
    "jpeg": "ğŸ–¼ï¸"
}

# ä¿é™©ç±»å‹é€‰é¡¹
INSURANCE_TYPES = [
    "å…¨éƒ¨",
    "Hull & Machinery",
    "Cargo",
    "P&I",
    "War Risk",
    "Marine Liability",
    "å…¶ä»–"
]

# =============================================================================
# ç³»ç»Ÿæç¤ºè¯ - ç®€åŒ–ç‰ˆ
# =============================================================================

ELECTRONIC_TEXT_SUMMARY_SYSTEM = """You are an underwriting document summarizer. 
Provide a BRIEF summary (3-5 sentences) of the document content covering:
- Insurance type and policy
- Insured party
- Key terms (premium, coverage, etc.)
- Main risk factors

Keep it concise and client-ready."""

HANDWRITING_TRANSLATION_SYSTEM = """You are a handwriting translator for insurance documents.

CRITICAL: For each handwritten annotation:
1. Translate to text (keep original language if unclear)
2. Estimate confidence (0-100%)
3. Describe location (e.g., "Top of page 1", "Margin right")

Output format:
[Location] Translated text (Confidence: XX%)

Example:
[Top of Page 1] To CEO: Renewal suggestions for your consideration (Confidence: 85%)
[Right margin, Page 2] Check premium calculation (Confidence: 92%)

DO NOT write a summary. Only translate each handwriting piece."""

# =============================================================================
# å·¥å…·å‡½æ•°
# =============================================================================

def ensure_directories():
    """ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„ç›®å½•å­˜åœ¨"""
    for dir_path in [DATA_DIR, WORKSPACES_DIR, EMBEDDINGS_DIR, 
                     ANALYSIS_DIR, REVIEW_DIR, AUDIT_DIR, CONFIG_DIR]:
        os.makedirs(dir_path, exist_ok=True)

def log_audit(action: str, details: Dict[str, Any]):
    """è®°å½•å®¡è®¡æ—¥å¿—"""
    timestamp = datetime.now().isoformat()
    log_entry = {
        "timestamp": timestamp,
        "action": action,
        "details": details
    }
    
    log_file = os.path.join(AUDIT_DIR, f"{datetime.now().strftime('%Y%m%d')}.json")
    
    logs = []
    if os.path.exists(log_file):
        with open(log_file, 'r', encoding='utf-8') as f:
            logs = json.load(f)
    
    logs.append(log_entry)
    
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(logs, f, indent=2, ensure_ascii=False)

def get_api_key() -> str:
    """è·å– API å¯†é’¥"""
    if "api_key" in st.session_state and st.session_state.api_key:
        return st.session_state.api_key
    return DEFAULT_API_KEY

def call_deepseek_api(messages: List[Dict[str, str]], max_tokens: int = 4000) -> Optional[str]:
    """è°ƒç”¨ DeepSeek API"""
    api_key = get_api_key()
    
    if not HAS_REQUESTS:
        return "Error: requests library not installed"
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    data = {
        "model": API_MODEL,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": 0.7
    }
    
    try:
        response = requests.post(
            f"{API_BASE}/chat/completions",
            headers=headers,
            json=data,
            timeout=60
        )
        response.raise_for_status()
        result = response.json()
        return result["choices"][0]["message"]["content"]
    except Exception as e:
        return f"API Error: {str(e)}"

# =============================================================================
# PDF å¤„ç†å‡½æ•°
# =============================================================================

def extract_text_from_pdf(file_path: str) -> Tuple[str, List[Dict]]:
    """ä» PDF æå–æ–‡æœ¬å’Œå›¾ç‰‡"""
    text = ""
    images = []
    
    if not HAS_PYMUPDF:
        return "Error: PyMuPDF not installed", []
    
    try:
        doc = fitz.open(file_path)
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            
            # æå–æ–‡æœ¬
            page_text = page.get_text()
            if page_text.strip():
                text += f"\n--- Page {page_num + 1} ---\n{page_text}"
            
            # æå–å›¾ç‰‡
            image_list = page.get_images(full=True)
            for img_index, img in enumerate(image_list):
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                
                # è½¬æ¢ä¸º base64
                image_b64 = base64.b64encode(image_bytes).decode()
                
                images.append({
                    "page": page_num + 1,
                    "index": img_index,
                    "format": base_image["ext"],
                    "data": image_b64,
                    "width": base_image.get("width", 0),
                    "height": base_image.get("height", 0)
                })
        
        doc.close()
        
        # æ£€æµ‹æ˜¯å¦æ˜¯æ‰«æä»¶
        if not text.strip() and images:
            text = f"[Scanned PDF detected: {len(images)} images found across {len(doc)} pages]"
        
        return text, images
        
    except Exception as e:
        return f"Error extracting PDF: {str(e)}", []

def detect_handwriting_in_images(images: List[Dict]) -> bool:
    """ç®€å•çš„å¯å‘å¼æ£€æµ‹ï¼šæ˜¯å¦å¯èƒ½åŒ…å«æ‰‹å†™å†…å®¹"""
    if not images:
        return False
    
    # å¯å‘å¼è§„åˆ™ï¼š
    # 1. å›¾ç‰‡è¾ƒå¤šï¼ˆå¯èƒ½æ˜¯æ‰«æä»¶ + æ‰‹å†™æ‰¹æ³¨ï¼‰
    # 2. æœ‰å°å°ºå¯¸å›¾ç‰‡ï¼ˆå¯èƒ½æ˜¯æ‰¹æ³¨ï¼‰
    
    if len(images) > 3:
        return True
    
    for img in images:
        # æ£€æŸ¥æ˜¯å¦æœ‰è¾ƒå°çš„å›¾ç‰‡ï¼ˆå¯èƒ½æ˜¯æ‰‹å†™æ‰¹æ³¨ï¼‰
        if img.get("width", 0) < 800 or img.get("height", 0) < 600:
            return True
    
    return False

# =============================================================================
# æ–‡æ¡£åˆ†æå‡½æ•°
# =============================================================================

def analyze_electronic_text(text: str) -> str:
    """åˆ†æç”µå­æ–‡æœ¬ï¼Œè¿”å›ç®€çŸ­æ‘˜è¦"""
    messages = [
        {"role": "system", "content": ELECTRONIC_TEXT_SUMMARY_SYSTEM},
        {"role": "user", "content": f"Summarize this insurance document:\n\n{text[:3000]}"}
    ]
    
    summary = call_deepseek_api(messages, max_tokens=500)
    return summary if summary else "Unable to generate summary"

def translate_handwriting(images: List[Dict], document_context: str = "") -> List[Dict]:
    """ç¿»è¯‘æ‰‹å†™å†…å®¹"""
    if not images:
        return []
    
    # æ„å»ºæç¤ºè¯
    context_info = f"Document context: {document_context[:500]}" if document_context else ""
    
    prompt = f"""Analyze the handwriting in this insurance document.
{context_info}

The document contains {len(images)} images across multiple pages.
For each handwritten annotation you can identify, provide:
1. Location (page number and position)
2. Translated text
3. Confidence percentage (0-100%)

Format:
[Location] Text (Confidence: XX%)"""

    messages = [
        {"role": "system", "content": HANDWRITING_TRANSLATION_SYSTEM},
        {"role": "user", "content": prompt}
    ]
    
    translation_result = call_deepseek_api(messages, max_tokens=2000)
    
    # è§£æç¿»è¯‘ç»“æœ
    translations = []
    
    if translation_result and "[" in translation_result:
        # ç®€å•è§£ææ ¼å¼: [Location] Text (Confidence: XX%)
        lines = translation_result.split('\n')
        for line in lines:
            if '[' in line and ']' in line:
                try:
                    # æå–ä½ç½®
                    location = line[line.find('[')+1:line.find(']')]
                    
                    # æå–æ–‡æœ¬å’Œç½®ä¿¡åº¦
                    rest = line[line.find(']')+1:].strip()
                    if '(Confidence:' in rest:
                        text = rest[:rest.find('(Confidence:')].strip()
                        conf_str = rest[rest.find('(Confidence:')+12:rest.find('%)')]
                        confidence = int(conf_str.strip())
                    else:
                        text = rest
                        confidence = 70  # é»˜è®¤å€¼
                    
                    translations.append({
                        "location": location,
                        "text": text,
                        "confidence": confidence,
                        "image_ref": None  # å¯ä»¥åç»­å…³è”åˆ°å…·ä½“å›¾ç‰‡
                    })
                except:
                    continue
    
    # å¦‚æœè§£æå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤ç¿»è¯‘
    if not translations and images:
        translations.append({
            "location": f"Page 1",
            "text": translation_result if translation_result else "Unable to translate handwriting",
            "confidence": 50,
            "image_ref": 0
        })
    
    return translations

# =============================================================================
# å·¥ä½œåŒºå’Œæ–‡æ¡£ç®¡ç†
# =============================================================================

def create_workspace(workspace_name: str, description: str = ""):
    """åˆ›å»ºå·¥ä½œåŒº"""
    workspace_dir = os.path.join(WORKSPACES_DIR, workspace_name)
    os.makedirs(workspace_dir, exist_ok=True)
    
    metadata = {
        "name": workspace_name,
        "description": description,
        "created_at": datetime.now().isoformat(),
        "documents": []
    }
    
    metadata_file = os.path.join(workspace_dir, "metadata.json")
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    log_audit("create_workspace", {"workspace": workspace_name})

def load_workspace_metadata(workspace_name: str) -> Optional[Dict]:
    """åŠ è½½å·¥ä½œåŒºå…ƒæ•°æ®"""
    metadata_file = os.path.join(WORKSPACES_DIR, workspace_name, "metadata.json")
    if os.path.exists(metadata_file):
        with open(metadata_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return None

def save_workspace_metadata(workspace_name: str, metadata: Dict):
    """ä¿å­˜å·¥ä½œåŒºå…ƒæ•°æ®"""
    metadata_file = os.path.join(WORKSPACES_DIR, workspace_name, "metadata.json")
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

def upload_document(workspace_name: str, uploaded_file, auto_analyze: bool = True) -> bool:
    """ä¸Šä¼ æ–‡æ¡£åˆ°å·¥ä½œåŒº"""
    try:
        # ä¿å­˜æ–‡ä»¶
        workspace_dir = os.path.join(WORKSPACES_DIR, workspace_name)
        file_path = os.path.join(workspace_dir, uploaded_file.name)
        
        with open(file_path, 'wb') as f:
            f.write(uploaded_file.getvalue())
        
        # æå–æ–‡æœ¬å’Œå›¾ç‰‡
        text, images = "", []
        if uploaded_file.name.lower().endswith('.pdf'):
            text, images = extract_text_from_pdf(file_path)
        
        # æ£€æµ‹æ‰‹å†™
        has_handwriting = detect_handwriting_in_images(images)
        
        # åˆ†ææ–‡æ¡£
        summary = ""
        handwriting_translations = []
        
        if auto_analyze and text:
            summary = analyze_electronic_text(text)
            
            if has_handwriting:
                handwriting_translations = translate_handwriting(images, text)
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_data = {
            "filename": uploaded_file.name,
            "upload_time": datetime.now().isoformat(),
            "has_images": len(images) > 0,
            "image_count": len(images),
            "has_handwriting": has_handwriting,
            "summary": summary,
            "handwriting_translations": handwriting_translations,
            "text_preview": text[:500] if text else ""
        }
        
        analysis_file = os.path.join(ANALYSIS_DIR, f"{uploaded_file.name}.json")
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜å›¾ç‰‡æ•°æ®
        if images:
            images_file = os.path.join(ANALYSIS_DIR, f"{uploaded_file.name}_images.json")
            with open(images_file, 'w', encoding='utf-8') as f:
                json.dump(images, f, indent=2)
        
        # æ›´æ–°å·¥ä½œåŒºå…ƒæ•°æ®
        metadata = load_workspace_metadata(workspace_name)
        if metadata:
            # æå–æ–‡æ¡£ä¿¡æ¯
            doc_info = {
                "filename": uploaded_file.name,
                "upload_time": datetime.now().isoformat(),
                "size": uploaded_file.size,
                "has_handwriting": has_handwriting,
                "insurance_type": extract_insurance_type(text, summary),
                "client_name": extract_client_name(text, summary),
                "underwriting_year": extract_year(text, summary)
            }
            
            metadata["documents"].append(doc_info)
            save_workspace_metadata(workspace_name, metadata)
        
        log_audit("upload_document", {
            "workspace": workspace_name,
            "filename": uploaded_file.name,
            "has_handwriting": has_handwriting
        })
        
        return True
        
    except Exception as e:
        st.error(f"Upload error: {str(e)}")
        return False

def extract_insurance_type(text: str, summary: str) -> str:
    """ä»æ–‡æœ¬ä¸­æå–ä¿é™©ç±»å‹"""
    combined = (text + " " + summary).lower()
    
    if "hull" in combined or "machinery" in combined:
        return "Hull & Machinery"
    elif "cargo" in combined:
        return "Cargo"
    elif "p&i" in combined or "protection" in combined:
        return "P&I"
    elif "war" in combined:
        return "War Risk"
    elif "liability" in combined:
        return "Marine Liability"
    else:
        return "å…¶ä»–"

def extract_client_name(text: str, summary: str) -> str:
    """ä»æ–‡æœ¬ä¸­æå–å®¢æˆ·åç§°"""
    # ç®€å•çš„æå–é€»è¾‘ï¼šæŸ¥æ‰¾å¸¸è§æ¨¡å¼
    combined = text + " " + summary
    
    # æŸ¥æ‰¾ "Insured:" æˆ–ç±»ä¼¼æ¨¡å¼
    patterns = [
        r"Insured[:\s]+([A-Z][a-zA-Z\s&]+)",
        r"Client[:\s]+([A-Z][a-zA-Z\s&]+)",
        r"Assured[:\s]+([A-Z][a-zA-Z\s&]+)"
    ]
    
    for pattern in patterns:
        match = re.search(pattern, combined)
        if match:
            return match.group(1).strip()[:50]
    
    return "Unknown"

def extract_year(text: str, summary: str) -> str:
    """ä»æ–‡æœ¬ä¸­æå–æ‰¿ä¿å¹´åº¦"""
    combined = text + " " + summary
    
    # æŸ¥æ‰¾å¹´ä»½ï¼ˆ2000-2030ï¼‰
    years = re.findall(r'20[0-2][0-9]', combined)
    if years:
        return years[0]
    
    return datetime.now().strftime("%Y")

# =============================================================================
# Streamlit UI
# =============================================================================

def render_table_view():
    """æ¸²æŸ“è¡¨æ ¼è§†å›¾"""
    st.header("ğŸ“Š Integrated Analysis Report")
    
    # è·å–æ‰€æœ‰å·¥ä½œåŒºçš„æ–‡æ¡£
    all_documents = []
    
    if os.path.exists(WORKSPACES_DIR):
        for workspace_name in os.listdir(WORKSPACES_DIR):
            metadata = load_workspace_metadata(workspace_name)
            if metadata and "documents" in metadata:
                for doc in metadata["documents"]:
                    doc["workspace"] = workspace_name
                    all_documents.append(doc)
    
    if not all_documents:
        st.info("ğŸ“­ No documents uploaded yet")
        return
    
    # ç­›é€‰æ§ä»¶
    col1, col2, col3 = st.columns(3)
    
    with col1:
        selected_type = st.selectbox(
            "ä¿é™©ç±»å‹",
            ["å…¨éƒ¨"] + list(set([d.get("insurance_type", "å…¶ä»–") for d in all_documents]))
        )
    
    with col2:
        selected_client = st.selectbox(
            "å®¢æˆ·åç§°",
            ["å…¨éƒ¨"] + list(set([d.get("client_name", "Unknown") for d in all_documents]))
        )
    
    with col3:
        selected_year = st.selectbox(
            "æ‰¿ä¿å¹´åº¦",
            ["å…¨éƒ¨"] + sorted(list(set([d.get("underwriting_year", "Unknown") for d in all_documents])), reverse=True)
        )
    
    # åº”ç”¨ç­›é€‰
    filtered_docs = all_documents
    
    if selected_type != "å…¨éƒ¨":
        filtered_docs = [d for d in filtered_docs if d.get("insurance_type") == selected_type]
    
    if selected_client != "å…¨éƒ¨":
        filtered_docs = [d for d in filtered_docs if d.get("client_name") == selected_client]
    
    if selected_year != "å…¨éƒ¨":
        filtered_docs = [d for d in filtered_docs if d.get("underwriting_year") == selected_year]
    
    # æ˜¾ç¤ºè¡¨æ ¼
    st.write(f"**å…± {len(filtered_docs)} ä¸ªæ–‡æ¡£**")
    
    if filtered_docs:
        # åˆ›å»ºè¡¨æ ¼æ•°æ®
        table_data = []
        for doc in filtered_docs:
            table_data.append({
                "æ¡ˆä¾‹åç§°": doc.get("filename", "Unknown"),
                "ç±»åˆ«": doc.get("insurance_type", "å…¶ä»–"),
                "æ‰¿ä¿å¹´åº¦": doc.get("underwriting_year", "Unknown"),
                "æœ€æ–°æ›´æ–°æ—¶é—´": doc.get("upload_time", "Unknown")[:19]
            })
        
        # æ˜¾ç¤ºä¸ºå¯ç‚¹å‡»çš„è¡¨æ ¼
        for idx, row in enumerate(table_data):
            with st.expander(f"ğŸ“„ {row['æ¡ˆä¾‹åç§°']} | {row['ç±»åˆ«']} | {row['æ‰¿ä¿å¹´åº¦']}"):
                col_a, col_b = st.columns([1, 3])
                
                with col_a:
                    st.write("**æ–‡æ¡£ä¿¡æ¯**")
                    st.write(f"- ç±»åˆ«: {row['ç±»åˆ«']}")
                    st.write(f"- æ‰¿ä¿å¹´åº¦: {row['æ‰¿ä¿å¹´åº¦']}")
                    st.write(f"- æ›´æ–°æ—¶é—´: {row['æœ€æ–°æ›´æ–°æ—¶é—´']}")
                
                with col_b:
                    # æ˜¾ç¤ºåˆ†æç»“æœ
                    render_document_analysis(filtered_docs[idx])

def render_document_analysis(doc_info: Dict):
    """æ¸²æŸ“å•ä¸ªæ–‡æ¡£çš„åˆ†æç»“æœ"""
    filename = doc_info.get("filename")
    
    # åŠ è½½åˆ†ææ•°æ®
    analysis_file = os.path.join(ANALYSIS_DIR, f"{filename}.json")
    if not os.path.exists(analysis_file):
        st.warning("åˆ†ææ•°æ®ä¸å­˜åœ¨")
        return
    
    with open(analysis_file, 'r', encoding='utf-8') as f:
        analysis_data = json.load(f)
    
    # æ˜¾ç¤ºç”µå­æ–‡æœ¬æ‘˜è¦
    st.write("**Electronic Text Analysis**")
    summary = analysis_data.get("summary", "No summary available")
    st.write(summary)
    
    # æ˜¾ç¤ºæ‰‹å†™ç¿»è¯‘
    if analysis_data.get("has_handwriting"):
        st.write("---")
        st.write("**Handwriting Translation**")
        
        translations = analysis_data.get("handwriting_translations", [])
        
        if translations:
            # åŠ è½½å›¾ç‰‡æ•°æ®
            images_file = os.path.join(ANALYSIS_DIR, f"{filename}_images.json")
            images = []
            if os.path.exists(images_file):
                with open(images_file, 'r', encoding='utf-8') as f:
                    images = json.load(f)
            
            for trans in translations:
                col_img, col_text = st.columns([1, 2])
                
                with col_img:
                    # æ˜¾ç¤ºç›¸å…³å›¾ç‰‡ï¼ˆå¦‚æœæœ‰ï¼‰
                    if images and trans.get("image_ref") is not None:
                        img_idx = trans["image_ref"]
                        if img_idx < len(images):
                            img_data = images[img_idx]["data"]
                            st.image(f"data:image/png;base64,{img_data}", width=200)
                    else:
                        st.write(f"ğŸ–¼ï¸ {trans['location']}")
                
                with col_text:
                    confidence = trans.get("confidence", 0)
                    st.write(f"**{trans['text']}**")
                    
                    # æ˜¾ç¤ºç½®ä¿¡åº¦æ¡
                    color = "green" if confidence >= 80 else "orange" if confidence >= 60 else "red"
                    st.progress(confidence / 100)
                    st.caption(f"è¯†åˆ«åº¦: {confidence}%")
        else:
            st.info("âœ… Have handwriting notes (ä¸Šä¼ æ‰‹å†™å›¾ç‰‡ä»¥è·å–ç¿»è¯‘)")

def main():
    """ä¸»å‡½æ•°"""
    st.set_page_config(
        page_title=APP_TITLE,
        page_icon="ğŸ“Š",
        layout="wide"
    )
    
    ensure_directories()
    
    st.title(f"{APP_TITLE} v{VERSION}")
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("âš™ï¸ é…ç½®")
        
        # API Key
        api_key = st.text_input(
            "API Key",
            value=get_api_key(),
            type="password",
            key="api_key_input"
        )
        
        if st.button("ğŸ’¾ ä¿å­˜"):
            st.session_state.api_key = api_key
            st.success("å·²ä¿å­˜")
        
        st.divider()
        
        # å·¥ä½œåŒºé€‰æ‹©
        st.header("ğŸ“ å·¥ä½œåŒº")
        
        workspaces = []
        if os.path.exists(WORKSPACES_DIR):
            workspaces = [d for d in os.listdir(WORKSPACES_DIR) 
                         if os.path.isdir(os.path.join(WORKSPACES_DIR, d))]
        
        if not workspaces:
            create_workspace("Default", "é»˜è®¤å·¥ä½œåŒº")
            workspaces = ["Default"]
        
        current_workspace = st.selectbox(
            "é€‰æ‹©å·¥ä½œåŒº",
            workspaces,
            key="current_workspace"
        )
        
        st.divider()
        
        # ä¸Šä¼ æ–‡ä»¶
        st.header("ğŸ“¤ ä¸Šä¼ æ–‡æ¡£")
        uploaded_file = st.file_uploader(
            "é€‰æ‹©æ–‡ä»¶",
            type=list(SUPPORTED_FORMATS.keys())
        )
        
        if uploaded_file:
            if st.button("ğŸš€ ä¸Šä¼ å¹¶åˆ†æ"):
                with st.spinner("å¤„ç†ä¸­..."):
                    success = upload_document(
                        current_workspace,
                        uploaded_file,
                        auto_analyze=True
                    )
                    if success:
                        st.success(f"âœ… å·²ä¸Šä¼ : {uploaded_file.name}")
                        st.rerun()
    
    # ä¸»å†…å®¹åŒº
    tab1, tab2 = st.tabs(["ğŸ“Š æŠ¥å‘Šè¡¨æ ¼", "â„¹ï¸ å…³äº"])
    
    with tab1:
        render_table_view()
    
    with tab2:
        st.header("å…³äºç³»ç»Ÿ")
        st.write(f"""
        **Enhanced Underwriting Assistant** v{VERSION}
        
        æ ¸å¿ƒåŠŸèƒ½ï¼š
        - âœ… è¡¨æ ¼å½¢å¼å±•ç¤ºæ‰€æœ‰æ–‡æ¡£
        - âœ… æ”¯æŒç­›é€‰ï¼ˆä¿é™©ç±»å‹ã€å®¢æˆ·åç§°ã€æ‰¿ä¿å¹´åº¦ï¼‰
        - âœ… ç”µå­æ–‡æœ¬ç®€çŸ­æ‘˜è¦
        - âœ… æ‰‹å†™ç¿»è¯‘ï¼ˆæ˜¾ç¤ºå›¾ç‰‡ + ç¿»è¯‘ + è¯†åˆ«åº¦ç™¾åˆ†æ¯”ï¼‰
        
        æŠ€æœ¯æ ˆï¼š
        - Streamlit
        - PyMuPDF (fitz)
        - DeepSeek API
        
        Powered by AI ğŸ¤–
        """)

if __name__ == "__main__":
    # åˆå§‹åŒ–æ•°æ®
    ensure_directories()
    
    # åŠ è½½åˆå§‹æ•°æ®é›†ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
    if not os.path.exists(os.path.join(WORKSPACES_DIR, "Default")):
        create_workspace("Default", "é»˜è®¤å·¥ä½œåŒº")
    
    main()
