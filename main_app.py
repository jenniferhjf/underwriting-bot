"""
Enhanced Underwriting RAG System - Main Application
====================================================
åŒ…å«ä¸¤ä¸ªä¸»è¦åŠŸèƒ½ï¼š
1. æ•°æ®é¢„å¤„ç†åŒº (Admin) - ä¸Šä¼ æ–‡æ¡£ã€é¢„å¤„ç†ã€æ„å»ºç´¢å¼•
2. RAGé—®ç­”åŒº (User) - é—®ç­”æ£€ç´¢ã€æ™ºèƒ½å›ç­”

Version: 1.0.0
Date: 2026-02-09
"""

import streamlit as st
import json
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    from modules.preprocessor import DocumentPreprocessor
    from modules.vector_store import VectorStore
    from modules.embeddings import EmbeddingGenerator
    from modules.llm_client import LLMClient
    from modules.rag_pipeline import RAGPipeline
except ImportError:
    st.error("âŒ æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œè¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–å·²å®‰è£…")
    st.stop()


# ============================================================================
# é…ç½®
# ============================================================================

class Config:
    """åº”ç”¨é…ç½®"""
    
    # æ•°æ®ç›®å½•
    DATA_DIR = Path("data")
    ELECTRONIC_DATA_FILE = DATA_DIR / "electronic_data.json"
    HANDWRITING_DATA_FILE = DATA_DIR / "handwriting_data.json"
    VECTOR_INDEX_FILE = DATA_DIR / "vector_index.faiss"
    METADATA_FILE = DATA_DIR / "metadata.json"
    
    # APIé…ç½®
    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
    DEEPSEEK_API_BASE = os.getenv("DEEPSEEK_API_BASE", "https://api.deepseek.com/v1")
    
    # Embeddingé…ç½®
    EMBEDDING_MODEL = "text-embedding-3-small"  # æˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹
    EMBEDDING_DIMENSION = 1536
    
    # RAGé…ç½®
    TOP_K = 5  # æ£€ç´¢Top-Kä¸ªç›¸å…³æ®µè½
    CHUNK_SIZE = 500  # æ–‡æœ¬åˆ†å—å¤§å°
    CHUNK_OVERLAP = 50  # åˆ†å—é‡å 
    
    @classmethod
    def init_storage(cls):
        """åˆå§‹åŒ–å­˜å‚¨ç›®å½•"""
        cls.DATA_DIR.mkdir(exist_ok=True)


Config.init_storage()


# ============================================================================
# ä¼šè¯çŠ¶æ€åˆå§‹åŒ–
# ============================================================================

def init_session_state():
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
    if 'preprocessor' not in st.session_state:
        st.session_state.preprocessor = DocumentPreprocessor()
    
    if 'vector_store' not in st.session_state:
        st.session_state.vector_store = VectorStore()
    
    if 'embedding_generator' not in st.session_state:
        st.session_state.embedding_generator = EmbeddingGenerator()
    
    if 'llm_client' not in st.session_state:
        st.session_state.llm_client = LLMClient()
    
    if 'rag_pipeline' not in st.session_state:
        st.session_state.rag_pipeline = RAGPipeline()
    
    if 'knowledge_base_loaded' not in st.session_state:
        st.session_state.knowledge_base_loaded = False
    
    if 'processing_status' not in st.session_state:
        st.session_state.processing_status = {}


# ============================================================================
# æ•°æ®é¢„å¤„ç†åŒº (æ­¥éª¤ 1-4)
# ============================================================================

def render_preprocessing_section():
    """æ¸²æŸ“æ•°æ®é¢„å¤„ç†åŒº"""
    st.header("ğŸ“¤ æ•°æ®é¢„å¤„ç†åŒº (Admin)")
    st.markdown("ä¸Šä¼ æ–‡æ¡£ï¼Œè¿›è¡Œé¢„å¤„ç†ï¼Œæ„å»ºçŸ¥è¯†åº“ç´¢å¼•")
    st.markdown("---")
    
    # Tab: ä¸Šä¼ ä¸é¢„å¤„ç† | ç´¢å¼•ç®¡ç† | æ•°æ®æŸ¥çœ‹
    tab1, tab2, tab3 = st.tabs(["ğŸ“„ ä¸Šä¼ ä¸é¢„å¤„ç†", "ğŸ” ç´¢å¼•ç®¡ç†", "ğŸ“Š æ•°æ®æŸ¥çœ‹"])
    
    with tab1:
        render_upload_and_process()
    
    with tab2:
        render_index_management()
    
    with tab3:
        render_data_viewer()


def render_upload_and_process():
    """ä¸Šä¼ æ–‡æ¡£å¹¶é¢„å¤„ç†"""
    st.subheader("æ­¥éª¤ 1-4: æ–‡æ¡£é¢„å¤„ç†æµç¨‹")
    
    st.markdown("""
    **å¤„ç†æµç¨‹ï¼š**
    1. ğŸ” è¯†åˆ«ç”µå­æ–‡æœ¬ vs æ‰‹å†™æ–‡æœ¬
    2. âœ‚ï¸ æ–‡æœ¬åˆ†å— (Chunking)
    3. ğŸ§® å‘é‡åŒ– (Embeddings)
    4. ğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“ + æ„å»ºç´¢å¼•
    """)
    
    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ æ–‡æ¡£ (PDF/DOCX)",
        type=["pdf", "docx"],
        accept_multiple_files=True,
        help="æ”¯æŒæ‰¹é‡ä¸Šä¼ å¤šä¸ªæ–‡æ¡£"
    )
    
    if uploaded_files:
        st.success(f"âœ… å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶")
        
        # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
        with st.expander("ğŸ“‹ æŸ¥çœ‹æ–‡ä»¶åˆ—è¡¨"):
            for f in uploaded_files:
                st.markdown(f"- **{f.name}** ({f.size / 1024:.2f} KB)")
        
        # å¤„ç†é…ç½®
        col1, col2 = st.columns(2)
        with col1:
            chunk_size = st.number_input("åˆ†å—å¤§å°", min_value=100, max_value=2000, value=Config.CHUNK_SIZE)
        with col2:
            chunk_overlap = st.number_input("åˆ†å—é‡å ", min_value=0, max_value=200, value=Config.CHUNK_OVERLAP)
        
        # å¼€å§‹å¤„ç†æŒ‰é’®
        if st.button("ğŸš€ å¼€å§‹é¢„å¤„ç†", type="primary"):
            process_documents(uploaded_files, chunk_size, chunk_overlap)


def process_documents(uploaded_files, chunk_size: int, chunk_overlap: int):
    """
    æ‰§è¡Œå®Œæ•´çš„æ–‡æ¡£é¢„å¤„ç†æµç¨‹
    æ­¥éª¤ 1: æ–‡ä»¶å¤„ç† (ç”µå­æ–‡æœ¬ vs æ‰‹å†™æ–‡æœ¬åˆ†ç¦»)
    æ­¥éª¤ 2: æ–‡æœ¬åˆ†å—
    æ­¥éª¤ 3: å‘é‡åŒ–
    æ­¥éª¤ 4: æ„å»ºç´¢å¼•
    """
    preprocessor = st.session_state.preprocessor
    embedding_generator = st.session_state.embedding_generator
    vector_store = st.session_state.vector_store
    
    # è¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    all_electronic_data = []
    all_handwriting_data = []
    
    for idx, uploaded_file in enumerate(uploaded_files):
        # æ›´æ–°è¿›åº¦
        progress = (idx + 1) / len(uploaded_files)
        progress_bar.progress(progress)
        status_text.text(f"å¤„ç†æ–‡ä»¶ {idx + 1}/{len(uploaded_files)}: {uploaded_file.name}")
        
        try:
            # === æ­¥éª¤ 1: æ–‡ä»¶å¤„ç†å’Œåˆ†ç¦» ===
            st.info(f"ğŸ“– æ­¥éª¤ 1/4: æå–å’Œåˆ†ç¦»å†…å®¹ - {uploaded_file.name}")
            
            file_bytes = uploaded_file.read()
            result = preprocessor.process_document(
                file_bytes=file_bytes,
                filename=uploaded_file.name
            )
            
            electronic_text = result.get("electronic_text", "")
            handwriting_images = result.get("handwriting_images", [])
            metadata = result.get("metadata", {})
            
            st.success(f"âœ… æ­¥éª¤ 1 å®Œæˆ: ç”µå­æ–‡æœ¬ {len(electronic_text)} å­—ç¬¦, æ‰‹å†™å›¾åƒ {len(handwriting_images)} å¼ ")
            
            # === æ­¥éª¤ 2: æ–‡æœ¬åˆ†å— ===
            if electronic_text:
                st.info(f"âœ‚ï¸ æ­¥éª¤ 2/4: æ–‡æœ¬åˆ†å— - {uploaded_file.name}")
                
                chunks = preprocessor.chunk_text(
                    text=electronic_text,
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )
                
                st.success(f"âœ… æ­¥éª¤ 2 å®Œæˆ: ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
                
                # === æ­¥éª¤ 3: å‘é‡åŒ– ===
                st.info(f"ğŸ§® æ­¥éª¤ 3/4: å‘é‡åŒ– - {uploaded_file.name}")
                
                embeddings = []
                for chunk in chunks:
                    embedding = embedding_generator.generate_embedding(chunk["text"])
                    chunk["embedding"] = embedding
                    embeddings.append(embedding)
                
                st.success(f"âœ… æ­¥éª¤ 3 å®Œæˆ: ç”Ÿæˆ {len(embeddings)} ä¸ªå‘é‡")
                
                # ä¿å­˜ç”µå­æ–‡æœ¬æ•°æ®
                doc_data = {
                    "doc_id": result["doc_id"],
                    "filename": uploaded_file.name,
                    "metadata": metadata,
                    "chunks": chunks
                }
                all_electronic_data.append(doc_data)
            
            # å¤„ç†æ‰‹å†™å›¾åƒ
            if handwriting_images:
                st.info(f"âœï¸ å¤„ç†æ‰‹å†™å›¾åƒ - {uploaded_file.name}")
                
                for img in handwriting_images:
                    # OCRè¯†åˆ«
                    ocr_result = preprocessor.perform_ocr(img["data"])
                    img["ocr_text"] = ocr_result["text"]
                    img["confidence"] = ocr_result["confidence"]
                    img["doc_id"] = result["doc_id"]
                
                all_handwriting_data.extend(handwriting_images)
                st.success(f"âœ… æ‰‹å†™å›¾åƒå¤„ç†å®Œæˆ: {len(handwriting_images)} å¼ ")
        
        except Exception as e:
            st.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {uploaded_file.name}")
            st.error(f"é”™è¯¯: {str(e)}")
            continue
    
    # === æ­¥éª¤ 4: æ„å»ºç´¢å¼•å¹¶ä¿å­˜ ===
    if all_electronic_data:
        st.info("ğŸ’¾ æ­¥éª¤ 4/4: ä¿å­˜æ•°æ®å¹¶æ„å»ºå‘é‡ç´¢å¼•")
        
        # ä¿å­˜JSONæ•°æ®
        save_processed_data(all_electronic_data, all_handwriting_data)
        
        # æ„å»ºå‘é‡ç´¢å¼•
        vector_store.build_index(all_electronic_data)
        
        st.success("âœ… æ­¥éª¤ 4 å®Œæˆ: æ•°æ®å·²ä¿å­˜ï¼Œå‘é‡ç´¢å¼•å·²æ„å»º")
        st.success(f"ğŸ“Š æ€»è®¡å¤„ç†: {len(all_electronic_data)} ä¸ªæ–‡æ¡£, {len(all_handwriting_data)} å¼ æ‰‹å†™å›¾åƒ")
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€
        st.session_state.knowledge_base_loaded = True
        st.balloons()
    else:
        st.warning("âš ï¸ æ²¡æœ‰æå–åˆ°æœ‰æ•ˆçš„ç”µå­æ–‡æœ¬æ•°æ®")
    
    progress_bar.empty()
    status_text.empty()


def save_processed_data(electronic_data: List[Dict], handwriting_data: List[Dict]):
    """ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®"""
    try:
        # ä¿å­˜ç”µå­æ–‡æœ¬æ•°æ®
        with open(Config.ELECTRONIC_DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump({
                "processed_at": datetime.now().isoformat(),
                "total_documents": len(electronic_data),
                "documents": electronic_data
            }, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æ‰‹å†™æ•°æ®
        with open(Config.HANDWRITING_DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump({
                "processed_at": datetime.now().isoformat(),
                "total_images": len(handwriting_data),
                "images": handwriting_data
            }, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "last_updated": datetime.now().isoformat(),
            "total_documents": len(electronic_data),
            "total_handwriting_images": len(handwriting_data)
        }
        with open(Config.METADATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2)
        
        st.success(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° {Config.DATA_DIR}")
    
    except Exception as e:
        st.error(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")


def render_index_management():
    """ç´¢å¼•ç®¡ç†"""
    st.subheader("ğŸ” å‘é‡ç´¢å¼•ç®¡ç†")
    
    # æ£€æŸ¥ç´¢å¼•çŠ¶æ€
    if Config.VECTOR_INDEX_FILE.exists():
        st.success("âœ… å‘é‡ç´¢å¼•å·²å­˜åœ¨")
        
        # æ˜¾ç¤ºç´¢å¼•ä¿¡æ¯
        vector_store = st.session_state.vector_store
        info = vector_store.get_index_info()
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ç´¢å¼•å‘é‡æ•°", info.get("num_vectors", "N/A"))
        with col2:
            st.metric("å‘é‡ç»´åº¦", info.get("dimension", "N/A"))
        with col3:
            st.metric("ç´¢å¼•ç±»å‹", info.get("index_type", "N/A"))
        
        # é‡å»ºç´¢å¼•
        if st.button("ğŸ”„ é‡å»ºç´¢å¼•"):
            with st.spinner("é‡å»ºç´¢å¼•ä¸­..."):
                rebuild_index()
    else:
        st.warning("âš ï¸ å‘é‡ç´¢å¼•ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿›è¡Œæ–‡æ¡£é¢„å¤„ç†")
        
        if st.button("ğŸ—ï¸ ä»ç°æœ‰æ•°æ®æ„å»ºç´¢å¼•"):
            if Config.ELECTRONIC_DATA_FILE.exists():
                with st.spinner("æ„å»ºç´¢å¼•ä¸­..."):
                    build_index_from_existing_data()
            else:
                st.error("âŒ æ²¡æœ‰æ‰¾åˆ°é¢„å¤„ç†æ•°æ®ï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£")


def rebuild_index():
    """é‡å»ºå‘é‡ç´¢å¼•"""
    try:
        if Config.ELECTRONIC_DATA_FILE.exists():
            with open(Config.ELECTRONIC_DATA_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            vector_store = st.session_state.vector_store
            vector_store.build_index(data["documents"])
            
            st.success("âœ… ç´¢å¼•é‡å»ºæˆåŠŸ")
        else:
            st.error("âŒ æ²¡æœ‰æ‰¾åˆ°æ•°æ®æ–‡ä»¶")
    except Exception as e:
        st.error(f"âŒ é‡å»ºç´¢å¼•å¤±è´¥: {e}")


def build_index_from_existing_data():
    """ä»ç°æœ‰æ•°æ®æ„å»ºç´¢å¼•"""
    rebuild_index()


def render_data_viewer():
    """æ•°æ®æŸ¥çœ‹å™¨"""
    st.subheader("ğŸ“Š æ•°æ®æŸ¥çœ‹")
    
    # é€‰æ‹©æ•°æ®ç±»å‹
    data_type = st.radio("é€‰æ‹©æ•°æ®ç±»å‹", ["ç”µå­æ–‡æœ¬", "æ‰‹å†™å›¾åƒ", "å…ƒæ•°æ®"])
    
    if data_type == "ç”µå­æ–‡æœ¬":
        if Config.ELECTRONIC_DATA_FILE.exists():
            with open(Config.ELECTRONIC_DATA_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            st.json(data, expanded=False)
            st.download_button(
                "ğŸ“¥ ä¸‹è½½ç”µå­æ–‡æœ¬æ•°æ®",
                data=json.dumps(data, indent=2, ensure_ascii=False),
                file_name="electronic_data.json",
                mime="application/json"
            )
        else:
            st.info("ğŸ“­ æš‚æ— ç”µå­æ–‡æœ¬æ•°æ®")
    
    elif data_type == "æ‰‹å†™å›¾åƒ":
        if Config.HANDWRITING_DATA_FILE.exists():
            with open(Config.HANDWRITING_DATA_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            st.markdown(f"**æ€»è®¡:** {data.get('total_images', 0)} å¼ å›¾åƒ")
            
            # æ˜¾ç¤ºå‰å‡ å¼ å›¾åƒ
            for idx, img in enumerate(data.get("images", [])[:5]):
                with st.expander(f"å›¾åƒ {idx + 1}"):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.markdown(f"**æ–‡æ¡£ID:** {img.get('doc_id', 'N/A')}")
                        st.markdown(f"**é¡µç :** {img.get('page', 'N/A')}")
                        st.markdown(f"**ç½®ä¿¡åº¦:** {img.get('confidence', 0) * 100:.1f}%")
                    with col2:
                        st.markdown("**OCRæ–‡æœ¬:**")
                        st.text(img.get('ocr_text', 'N/A')[:200])
        else:
            st.info("ğŸ“­ æš‚æ— æ‰‹å†™å›¾åƒæ•°æ®")
    
    else:  # å…ƒæ•°æ®
        if Config.METADATA_FILE.exists():
            with open(Config.METADATA_FILE, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            st.json(metadata)
        else:
            st.info("ğŸ“­ æš‚æ— å…ƒæ•°æ®")


# ============================================================================
# RAG é—®ç­”åŒº (æ­¥éª¤ 5-8)
# ============================================================================

def render_rag_section():
    """æ¸²æŸ“RAGé—®ç­”åŒº"""
    st.header("ğŸ’¬ RAG é—®ç­”åŒº (User)")
    st.markdown("åŸºäºçŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    st.markdown("---")
    
    # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å·²åŠ è½½
    if not check_knowledge_base():
        st.warning("âš ï¸ çŸ¥è¯†åº“å°šæœªåŠ è½½æˆ–ä¸å­˜åœ¨")
        st.info("ğŸ‘‰ è¯·å…ˆåœ¨ **æ•°æ®é¢„å¤„ç†åŒº** ä¸Šä¼ æ–‡æ¡£å¹¶å®Œæˆé¢„å¤„ç†")
        return
    
    # åŠ è½½çŸ¥è¯†åº“
    if not st.session_state.knowledge_base_loaded:
        with st.spinner("ğŸ“š åŠ è½½çŸ¥è¯†åº“ä¸­..."):
            load_knowledge_base()
    
    st.success("âœ… çŸ¥è¯†åº“å·²åŠ è½½")
    
    # æ˜¾ç¤ºçŸ¥è¯†åº“ç»Ÿè®¡
    display_kb_stats()
    
    st.markdown("---")
    
    # é—®ç­”ç•Œé¢
    render_qa_interface()


def check_knowledge_base() -> bool:
    """æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨"""
    return (Config.ELECTRONIC_DATA_FILE.exists() and 
            Config.VECTOR_INDEX_FILE.exists())


def load_knowledge_base():
    """åŠ è½½çŸ¥è¯†åº“åˆ°å†…å­˜"""
    try:
        # åŠ è½½å‘é‡å­˜å‚¨
        vector_store = st.session_state.vector_store
        vector_store.load_index(Config.VECTOR_INDEX_FILE)
        
        # åŠ è½½æ–‡æ¡£æ•°æ®
        with open(Config.ELECTRONIC_DATA_FILE, 'r', encoding='utf-8') as f:
            electronic_data = json.load(f)
        
        st.session_state.electronic_data = electronic_data
        st.session_state.knowledge_base_loaded = True
        
        st.success("âœ… çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
    
    except Exception as e:
        st.error(f"âŒ åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
        st.session_state.knowledge_base_loaded = False


def display_kb_stats():
    """æ˜¾ç¤ºçŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
    try:
        with open(Config.METADATA_FILE, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ğŸ“„ æ–‡æ¡£æ•°", metadata.get("total_documents", 0))
        with col2:
            st.metric("âœï¸ æ‰‹å†™å›¾åƒ", metadata.get("total_handwriting_images", 0))
        with col3:
            last_updated = metadata.get("last_updated", "N/A")
            if last_updated != "N/A":
                last_updated = last_updated.split("T")[0]
            st.metric("ğŸ”„ æœ€åæ›´æ–°", last_updated)
    
    except Exception as e:
        st.warning(f"æ— æ³•åŠ è½½ç»Ÿè®¡ä¿¡æ¯: {e}")


def render_qa_interface():
    """é—®ç­”ç•Œé¢"""
    st.subheader("ğŸ¤” æå‡ºæ‚¨çš„é—®é¢˜")
    
    # é—®é¢˜è¾“å…¥
    query = st.text_area(
        "è¾“å…¥æ‚¨çš„é—®é¢˜:",
        height=100,
        placeholder="ä¾‹å¦‚ï¼šè¿™ä»½ä¿å•çš„æ‰¿ä¿èŒƒå›´æ˜¯ä»€ä¹ˆï¼Ÿ",
        help="è¾“å…¥å…³äºä¿é™©æ–‡æ¡£çš„é—®é¢˜"
    )
    
    # é«˜çº§é€‰é¡¹
    with st.expander("âš™ï¸ é«˜çº§é€‰é¡¹"):
        col1, col2 = st.columns(2)
        with col1:
            top_k = st.slider("æ£€ç´¢Top-K", min_value=1, max_value=10, value=Config.TOP_K)
        with col2:
            show_sources = st.checkbox("æ˜¾ç¤ºæ¥æº", value=True)
    
    # æäº¤æŒ‰é’®
    if st.button("ğŸ” æœç´¢ç­”æ¡ˆ", type="primary"):
        if query.strip():
            with st.spinner("ğŸ¤” æ€è€ƒä¸­..."):
                answer_question(query, top_k, show_sources)
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥é—®é¢˜")


def answer_question(query: str, top_k: int, show_sources: bool):
    """
    æ‰§è¡Œå®Œæ•´çš„RAGæµç¨‹å›ç­”é—®é¢˜
    æ­¥éª¤ 5: é—®é¢˜å‘é‡åŒ–
    æ­¥éª¤ 6: çŸ¥è¯†åº“æ£€ç´¢
    æ­¥éª¤ 7: ç»„è£…Context
    æ­¥éª¤ 8: LLMç”Ÿæˆç­”æ¡ˆ
    """
    try:
        rag_pipeline = st.session_state.rag_pipeline
        
        # === æ­¥éª¤ 5: é—®é¢˜å‘é‡åŒ– ===
        st.info("ğŸ§® æ­¥éª¤ 5/8: é—®é¢˜å‘é‡åŒ–...")
        query_embedding = st.session_state.embedding_generator.generate_embedding(query)
        st.success("âœ… æ­¥éª¤ 5 å®Œæˆ")
        
        # === æ­¥éª¤ 6: çŸ¥è¯†åº“æ£€ç´¢ ===
        st.info(f"ğŸ” æ­¥éª¤ 6/8: æ£€ç´¢Top-{top_k}ç›¸å…³æ®µè½...")
        vector_store = st.session_state.vector_store
        search_results = vector_store.search(query_embedding, top_k=top_k)
        st.success(f"âœ… æ­¥éª¤ 6 å®Œæˆ: æ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³æ®µè½")
        
        # === æ­¥éª¤ 7: ç»„è£…Context ===
        st.info("ğŸ“ æ­¥éª¤ 7/8: ç»„è£…ä¸Šä¸‹æ–‡...")
        context = rag_pipeline.build_context(search_results)
        st.success("âœ… æ­¥éª¤ 7 å®Œæˆ")
        
        # === æ­¥éª¤ 8: LLMç”Ÿæˆç­”æ¡ˆ ===
        st.info("ğŸ¤– æ­¥éª¤ 8/8: ç”Ÿæˆç­”æ¡ˆ...")
        llm_client = st.session_state.llm_client
        answer = llm_client.generate_answer(query, context)
        st.success("âœ… æ­¥éª¤ 8 å®Œæˆ")
        
        # æ˜¾ç¤ºç­”æ¡ˆ
        st.markdown("---")
        st.subheader("ğŸ’¡ ç­”æ¡ˆ")
        st.markdown(answer)
        
        # æ˜¾ç¤ºæ¥æº
        if show_sources:
            st.markdown("---")
            st.subheader("ğŸ“š ç›¸å…³æ¥æº")
            for idx, result in enumerate(search_results):
                with st.expander(f"æ¥æº {idx + 1}: {result['filename']} (ç›¸ä¼¼åº¦: {result['score']:.3f})"):
                    st.markdown(f"**é¡µç :** {result.get('page', 'N/A')}")
                    st.markdown(f"**å†…å®¹:**")
                    st.text(result['text'][:500] + "..." if len(result['text']) > 500 else result['text'])
    
    except Exception as e:
        st.error(f"âŒ å›ç­”é—®é¢˜å¤±è´¥: {e}")
        if Config.DEBUG_MODE:
            st.exception(e)


# ============================================================================
# ä¸»åº”ç”¨
# ============================================================================

def main():
    """ä¸»åº”ç”¨å…¥å£"""
    
    # é¡µé¢é…ç½®
    st.set_page_config(
        page_title="Enhanced Underwriting RAG System",
        page_icon="ğŸ”",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    init_session_state()
    
    # æ ‡é¢˜
    st.title("ğŸ” Enhanced Underwriting RAG System")
    st.markdown("### AIé©±åŠ¨çš„ä¿é™©æ–‡æ¡£çŸ¥è¯†åº“ä¸æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    
    # ä¾§è¾¹æ  - æ¨¡å¼é€‰æ‹©
    st.sidebar.title("ğŸ“‚ åŠŸèƒ½é€‰æ‹©")
    mode = st.sidebar.radio(
        "é€‰æ‹©æ¨¡å¼",
        ["ğŸ’¬ RAG é—®ç­”åŒº", "ğŸ“¤ æ•°æ®é¢„å¤„ç†åŒº"],
        index=0
    )
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ“Š ç³»ç»ŸçŠ¶æ€")
    
    # æ˜¾ç¤ºçŸ¥è¯†åº“çŠ¶æ€
    if check_knowledge_base():
        st.sidebar.success("âœ… çŸ¥è¯†åº“å·²å°±ç»ª")
    else:
        st.sidebar.warning("âš ï¸ çŸ¥è¯†åº“æœªå°±ç»ª")
    
    # æ˜¾ç¤ºAPIçŠ¶æ€
    if Config.DEEPSEEK_API_KEY:
        st.sidebar.success("âœ… APIå·²é…ç½®")
    else:
        st.sidebar.warning("âš ï¸ APIæœªé…ç½®")
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("### â„¹ï¸ ç³»ç»Ÿä¿¡æ¯")
    st.sidebar.markdown(f"**ç‰ˆæœ¬:** 1.0.0")
    st.sidebar.markdown(f"**æ›´æ–°:** 2026-02-09")
    
    # ä¸»è¦å†…å®¹åŒºåŸŸ
    st.markdown("---")
    
    if mode == "ğŸ“¤ æ•°æ®é¢„å¤„ç†åŒº":
        render_preprocessing_section()
    else:
        render_rag_section()
    
    # é¡µè„š
    st.markdown("---")
    st.markdown(
        "<div style='text-align: center; color: gray;'>"
        "Enhanced Underwriting RAG System v1.0.0 | "
        "Built with Streamlit & DeepSeek"
        "</div>",
        unsafe_allow_html=True
    )


if __name__ == "__main__":
    main()
